[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Notebook",
    "section": "",
    "text": "Topic 1 : Predictive Analysis To Predict Diagnosis of a Breast Tumor\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTopic 2 : Clustering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTopic 3 : Linear and Nonlinear regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTopic 4: Classification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTopic 5: Anomaly Detection\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "My Github Repository",
    "section": "",
    "text": "Link below"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Predictive Analysis",
    "section": "",
    "text": "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed."
  },
  {
    "objectID": "posts/Predictive Analysis/index.html",
    "href": "posts/Predictive Analysis/index.html",
    "title": "Predictive Analysis",
    "section": "",
    "text": "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Predictive Analysis",
    "section": "",
    "text": "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed."
  },
  {
    "objectID": "posts/Anomaly Detection/index.html",
    "href": "posts/Anomaly Detection/index.html",
    "title": "Predictive Analysis",
    "section": "",
    "text": "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Linear regression is a fundamental supervised machine learning technique used for predicting a continuous outcome variable (also called the dependent variable) based on one or more predictor variables (independent variables). It assumes a linear relationship between the predictor variables and the target variable, meaning that it tries to find a linear equation that best fits the data."
  },
  {
    "objectID": "posts/Anomaly Detection/Anomaly.html",
    "href": "posts/Anomaly Detection/Anomaly.html",
    "title": "Topic 5: Anomaly Detection",
    "section": "",
    "text": "Anomaly (or outlier) detection is the data-driven task of identifying these rare occurrences and filtering or modulating them from the analysis pipeline. Such anomalous events can be connected to some fault in the data source, such as financial fraud, equipment fault, or irregularities in time series analysisAnomaly detection is an important task in machine learning, and various techniques can be employed depending on the nature of the data and the availability of labeled examples. \n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.svm import OneClassSVM\n \n\n\ndata = pd.read_csv(\"./iris.csv\")\ndf = data[[\"sepal_length\", \"sepal_width\"]]\nprint(df.describe())\n\n       sepal_length  sepal_width\ncount    150.000000   150.000000\nmean       5.843333     3.054000\nstd        0.828066     0.433594\nmin        4.300000     2.000000\n25%        5.100000     2.800000\n50%        5.800000     3.000000\n75%        6.400000     3.300000\nmax        7.900000     4.400000\n\n\nPredicted outliers from this data:\n\nmodel = OneClassSVM(kernel = 'rbf' , gamma = 0.001, nu = 0.05).fit(df)\ny_pred = model.predict(df)\n\n\nanomaly_values = df.iloc[np.where(y_pred == -1)]\nprint(anomaly_values)\n\n     sepal_length  sepal_width\n8             4.4          2.9\n13            4.3          3.0\n41            4.5          2.3\n117           7.7          3.8\n118           7.7          2.6\n131           7.9          3.8\n\n\n\nplt.scatter(df[\"sepal_length\"], df[\"sepal_width\"])\nplt.scatter(anomaly_values[\"sepal_length\"], anomaly_values[\"sepal_width\"], c = \"r\")\n\n&lt;matplotlib.collections.PathCollection at 0x266e0e53710&gt;\n\n\n\n\n\n\nplt.scatter(df[\"sepal_length\"], df[\"sepal_width\"])\nplt.scatter(anomaly_values[\"sepal_length\"], anomaly_values[\"sepal_width\"], c = \"r\") \n\n&lt;matplotlib.collections.PathCollection at 0x266e0ecf650&gt;\n\n\n\n\n\nAnomaly Detection for Credit Card Dataset\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize \nfrom sklearn.decomposition import PCA\n\n\nX = pd.read_csv(\".//CC GENERAL.csv\")\nX = X.drop('CUST_ID', axis = 1)\n\nX.fillna(method = 'ffill', inplace = True)\n\nprint(X.head())\n\n       BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \\\n0    40.900749           0.818182      95.40              0.00   \n1  3202.467416           0.909091       0.00              0.00   \n2  2495.148862           1.000000     773.17            773.17   \n3  1666.670542           0.636364    1499.00           1499.00   \n4   817.714335           1.000000      16.00             16.00   \n\n   INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n0                    95.4      0.000000             0.166667   \n1                     0.0   6442.945483             0.000000   \n2                     0.0      0.000000             1.000000   \n3                     0.0    205.788017             0.083333   \n4                     0.0      0.000000             0.083333   \n\n   ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n0                    0.000000                          0.083333   \n1                    0.000000                          0.000000   \n2                    1.000000                          0.000000   \n3                    0.083333                          0.000000   \n4                    0.083333                          0.000000   \n\n   CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n0                0.000000                 0              2        1000.0   \n1                0.250000                 4              0        7000.0   \n2                0.000000                 0             12        7500.0   \n3                0.083333                 1              1        7500.0   \n4                0.000000                 0              1        1200.0   \n\n      PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  \n0   201.802084        139.509787          0.000000      12  \n1  4103.032597       1072.340217          0.222222      12  \n2   622.066742        627.284787          0.000000      12  \n3     0.000000        627.284787          0.000000      12  \n4   678.334763        244.791237          0.000000      12  \n\n\n\nprint(X.columns)\nprint(f\"number of rows: {len(X)}\")\n\nIndex(['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES',\n       'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY',\n       'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',\n       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',\n       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT',\n       'TENURE'],\n      dtype='object')\nnumber of rows: 8950\n\n\n\nX.describe()\n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\ncount\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n\n\nmean\n1564.474828\n0.877271\n1003.204834\n592.437371\n411.067645\n978.871112\n0.490351\n0.202458\n0.364437\n0.135144\n3.248827\n14.709832\n4494.394205\n1733.143852\n865.225790\n0.153715\n11.517318\n\n\nstd\n2081.531879\n0.236904\n2136.634782\n1659.887917\n904.338115\n2097.163877\n0.401371\n0.298336\n0.397448\n0.200121\n6.824647\n24.857649\n3638.616165\n2895.063757\n2376.929826\n0.292499\n1.338331\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n50.000000\n0.000000\n0.019163\n0.000000\n6.000000\n\n\n25%\n128.281915\n0.888889\n39.635000\n0.000000\n0.000000\n0.000000\n0.083333\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1600.000000\n383.276166\n169.488256\n0.000000\n12.000000\n\n\n50%\n873.385231\n1.000000\n361.280000\n38.000000\n89.000000\n0.000000\n0.500000\n0.083333\n0.166667\n0.000000\n0.000000\n7.000000\n3000.000000\n856.901546\n312.096808\n0.000000\n12.000000\n\n\n75%\n2054.140036\n1.000000\n1110.130000\n577.405000\n468.637500\n1113.821139\n0.916667\n0.300000\n0.750000\n0.222222\n4.000000\n17.000000\n6500.000000\n1901.134317\n815.375602\n0.142857\n12.000000\n\n\nmax\n19043.138560\n1.000000\n49039.570000\n40761.250000\n22500.000000\n47137.211760\n1.000000\n1.000000\n1.000000\n1.500000\n123.000000\n358.000000\n30000.000000\n50721.483360\n76406.207520\n1.000000\n12.000000\n\n\n\n\n\n\n\n\nscaler = StandardScaler()\n\nX_s = scaler.fit_transform(X)\nX_norm = pd.DataFrame(normalize(X_s))\n\n\npca = PCA(n_components = 2) \nX_reduce = pca.fit_transform(X_norm)\nX_reduce = pd.DataFrame(X_reduce)\nX_reduce.columns = list([f'P{i}' for i in range(1, len(X_reduce.columns)+1)])\n\n\n\ndb_model = DBSCAN(eps = 0.05, min_samples = 10).fit(X_reduce)\nlabels = db_model.labels_\n\n\nnp.unique(labels)\n\narray([-1,  0,  1], dtype=int64)\n\n\n\nnp.histogram(labels, bins=len(np.unique(labels)))\n\n(array([  39, 8903,    8], dtype=int64),\n array([-1.        , -0.33333333,  0.33333333,  1.        ]))\n\n\n\nplt.hist(labels, bins=len(np.unique(labels)), log=True)\nplt.show()\n\n\n\n\n\nn_clusters = len(np.unique(labels))-1\nanomaly = list(labels).count(-1)\nprint(f'Clusters: {n_clusters}')\nprint(f'Abnormal points: {anomaly}')\n\nClusters: 2\nAbnormal points: 39\n\n\n\nX_anomaly = X.iloc[np.argwhere(labels==-1).reshape((-1,))]\nprint(X_anomaly.head())\n\n         BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \\\n86   7069.950386                1.0    1603.78           1445.14   \n87   8181.251131                1.0    2258.01           1318.78   \n109  6644.201651                1.0    4478.75           2721.59   \n120  8504.876253                1.0    6724.26           4100.08   \n468  6426.639738                1.0    4462.86           2816.46   \n\n     INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n86                   158.64   5626.004046                  1.0   \n87                   939.23   5251.228934                  1.0   \n109                 1757.16   7205.520805                  1.0   \n120                 2624.18   1686.599777                  1.0   \n468                 1646.40   4599.625146                  1.0   \n\n     ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n86                     0.833333                          1.000000   \n87                     0.750000                          1.000000   \n109                    0.916667                          1.000000   \n120                    0.916667                          1.000000   \n468                    0.833333                          0.916667   \n\n     CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n86                 0.750000                23             23       11000.0   \n87                 0.750000                21             60       13500.0   \n109                0.583333                21             64        9000.0   \n120                0.500000                10             85       17000.0   \n468                0.333333                11            103       11800.0   \n\n         PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  \n86    4589.873897       1876.262988          0.000000      12  \n87    3021.575846       2074.581541          0.000000      12  \n109  10857.943210       2469.571326          0.083333      12  \n120   1888.756861       2167.446204          0.000000      12  \n468   1793.043228       3101.017030          0.000000      12  \n\n\n\ncolours = {}\ncolours[0] = 'r'\ncolours[1] = 'g'\ncolours[2] = 'b'\ncolours[-1] = 'k'\n\ncvec = [colours[label] for label in labels]\n\nr = plt.scatter(X_reduce['P1'], X_reduce['P2'], color ='r');\ng = plt.scatter(X_reduce['P1'], X_reduce['P2'], color ='g');\nb = plt.scatter(X_reduce['P1'], X_reduce['P2'], color ='b');\nk = plt.scatter(X_reduce['P1'], X_reduce['P2'], color ='k');\n\nplt.figure(figsize =(9, 9))\nplt.scatter(X_reduce['P1'], X_reduce['P2'], c = cvec)\n\nplt.legend((r, g, b, k), ('Label 0', 'Label 1', 'Label 2', 'Label -1'))\nplt.show()"
  },
  {
    "objectID": "posts/Regression/Linear_Nonlinear_Regression .html",
    "href": "posts/Regression/Linear_Nonlinear_Regression .html",
    "title": "Topic 3 : Linear and Nonlinear regression",
    "section": "",
    "text": "Linear regression is a fundamental supervised machine learning technique used for predicting a continuous outcome variable (also called the dependent variable) based on one or more predictor variables (independent variables). It assumes a linear relationship between the predictor variables and the target variable, meaning that it tries to find a linear equation that best fits the data.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx = np.arange(-5.0, 5.0, 0.1)\n\n# You can adjust the slope and intercept\n# to verify the changes in the graph\ny = 2*(x) + 3\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n# plt.figure(figsize =(8, 6))\nplt.plot(x, ydata, 'bo')\nplt.plot(x, y, 'r')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.show()\n\n\n\n\n\n\nNon-Linear regression is a type of polynomial regression. It is a method to model a non-linear relationship between the dependent and independent variables. It is used in place when the data shows a curvy trend and linear regression would not produce very accurate results when compared to non-linear regression. This is because in linear regression it is pre-assumed that the data is linear.\n\n# compare the effect of the degree on the number of created features\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom matplotlib import pyplot\n\n# get the dataset\ndef get_dataset():\n    # load dataset\n    url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n    dataset = read_csv(url, header=None)\n    data = dataset.values\n    # separate into input and output columns\n    X, y = data[:, :-1], data[:, -1]\n    # ensure inputs are floats and output is an integer label\n    X = X.astype('float32')\n    y = LabelEncoder().fit_transform(y.astype('str'))\n    return X, y\n\n# define dataset\nX, y = get_dataset()\n# calculate change in number of features\nnum_features = list()\ndegress = [i for i in range(1, 6)]\nfor d in degress:\n    # create transform\n    trans = PolynomialFeatures(degree=d)\n    # fit and transform\n    data = trans.fit_transform(X)\n    # record number of features\n    num_features.append(data.shape[1])\n    # summarize\n    print('Degree: %d, Features: %d' % (d, data.shape[1]))\n# plot degree vs number of features\npyplot.plot(degress, num_features)\nplt.ylabel('The Number of Input Features')\nplt.xlabel('Degree')\npyplot.show()\n\nDegree: 1, Features: 61\nDegree: 2, Features: 1891\nDegree: 3, Features: 39711\nDegree: 4, Features: 635376\nDegree: 5, Features: 8259888"
  },
  {
    "objectID": "posts/Classification/Classification.html",
    "href": "posts/Classification/Classification.html",
    "title": "Topic 4: Classification",
    "section": "",
    "text": "Classification is a versatile machine learning technique used in various applications, such as loan application, spam detection, image recognition, sentiment analysis, medical diagnosis, and many more. It is a supervised learning task because it relies on labeled data to learn and make predictions.\n\n\nDistribution of Loans\n\nimport pandas as pd\n%matplotlib inline\nloan_data = pd.read_csv(\"loan_data.csv\")\nloan_data.head()\n\n\n\n\n\n\n\n\ncredit.policy\npurpose\nint.rate\ninstallment\nlog.annual.inc\ndti\nfico\ndays.with.cr.line\nrevol.bal\nrevol.util\ninq.last.6mths\ndelinq.2yrs\npub.rec\nnot.fully.paid\n\n\n\n\n0\n1\ndebt_consolidation\n0.1189\n829.10\n11.350407\n19.48\n737\n5639.958333\n28854\n52.1\n0\n0\n0\n0\n\n\n1\n1\ncredit_card\n0.1071\n228.22\n11.082143\n14.29\n707\n2760.000000\n33623\n76.7\n0\n0\n0\n0\n\n\n2\n1\ndebt_consolidation\n0.1357\n366.86\n10.373491\n11.63\n682\n4710.000000\n3511\n25.6\n1\n0\n0\n0\n\n\n3\n1\ndebt_consolidation\n0.1008\n162.34\n11.350407\n8.10\n712\n2699.958333\n33667\n73.2\n1\n0\n0\n0\n\n\n4\n1\ncredit_card\n0.1426\n102.92\n11.299732\n14.97\n667\n4066.000000\n4740\n39.5\n0\n1\n0\n0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n# Helper function for data distribution\n# Visualize the proportion of borrowers\ndef show_loan_distrib(data):\n  count = \"\"\n  if isinstance(data, pd.DataFrame):\n      count = data[\"not.fully.paid\"].value_counts()\n  else:\n      count = data.value_counts()\n\n\n  count.plot(kind = 'pie', explode = [0, 0.1], \n\n              figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n  plt.ylabel(\"Loan: Fully Paid Vs. Not Fully Paid\")\n  plt.legend([\"Fully Paid\", \"Not Fully Paid\"])\n  plt.show()\n  # Visualize the proportion of borrowers\nshow_loan_distrib(loan_data)  \n\n\n\n\n\nVariable Types\n\n# Check column types\nprint(loan_data.dtypes)\n\ncredit.policy          int64\npurpose               object\nint.rate             float64\ninstallment          float64\nlog.annual.inc       float64\ndti                  float64\nfico                   int64\ndays.with.cr.line    float64\nrevol.bal              int64\nrevol.util           float64\ninq.last.6mths         int64\ndelinq.2yrs            int64\npub.rec                int64\nnot.fully.paid         int64\ndtype: object\n\n\n\n# Check column types\nprint(loan_data.dtypes)\n\ncredit.policy          int64\npurpose               object\nint.rate             float64\ninstallment          float64\nlog.annual.inc       float64\ndti                  float64\nfico                   int64\ndays.with.cr.line    float64\nrevol.bal              int64\nrevol.util           float64\ninq.last.6mths         int64\ndelinq.2yrs            int64\npub.rec                int64\nnot.fully.paid         int64\ndtype: object\n\n\n\nencoded_loan_data = pd.get_dummies(loan_data, prefix=\"purpose\",   \n\n                                   drop_first=True)\nprint(encoded_loan_data.dtypes)\n\ncredit.policy                   int64\nint.rate                      float64\ninstallment                   float64\nlog.annual.inc                float64\ndti                           float64\nfico                            int64\ndays.with.cr.line             float64\nrevol.bal                       int64\nrevol.util                    float64\ninq.last.6mths                  int64\ndelinq.2yrs                     int64\npub.rec                         int64\nnot.fully.paid                  int64\npurpose_credit_card             uint8\npurpose_debt_consolidation      uint8\npurpose_educational             uint8\npurpose_home_improvement        uint8\npurpose_major_purchase          uint8\npurpose_small_business          uint8\ndtype: object\n\n\n\nX = encoded_loan_data.drop('not.fully.paid', axis = 1)\ny = encoded_loan_data['not.fully.paid']\n\n\n\n!pip install scikit-learn\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,stratify = y, random_state=2022)\n\nRequirement already satisfied: scikit-learn in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (1.3.0)\nRequirement already satisfied: numpy&gt;=1.17.3 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (1.24.3)\nRequirement already satisfied: scipy&gt;=1.5.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (2.2.0)\n\n\n\nX_train_cp = X_train.copy()\nX_train_cp['not.fully.paid'] = y_train\ny_0 = X_train_cp[X_train_cp['not.fully.paid'] == 0]\ny_1 = X_train_cp[X_train_cp['not.fully.paid'] == 1]\ny_0_undersample = y_0.sample(y_1.shape[0])\nloan_data_undersample = pd.concat([y_0_undersample, y_1], axis = 0)\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data_undersample)\n\n\n\n\n\n!pip install -U imbalanced-learn\n# check version number\nfrom imblearn import over_sampling\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy='minority')\nX_train_SMOTE, y_train_SMOTE = smote.fit_resample(X_train,y_train)\n# Visualize the proportion of borrowers\nshow_loan_distrib(y_train_SMOTE)\n\nRequirement already satisfied: imbalanced-learn in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (0.11.0)\nRequirement already satisfied: numpy&gt;=1.17.3 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.24.3)\nRequirement already satisfied: scipy&gt;=1.5.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.10.1)\nRequirement already satisfied: scikit-learn&gt;=1.0.2 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.3.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n\n\nImportError: cannot import name '_check_X' from 'imblearn.utils._validation' (C:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\imblearn\\utils\\_validation.py)\n\n\n\nX = loan_data_undersample.drop('not.fully.paid', axis = 1)\ny = loan_data_undersample['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify = y, random_state=2022)\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(X_train, y_train)\ny_pred = logistic_classifier.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))"
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "Topic 2 : Clustering",
    "section": "",
    "text": "Clustering algorithms are used to group data points based on certain similarities. Thereâ€™s no criterion for good clustering. Clustering determines the grouping with unlabelled data. It mainly depends on the specific user and the scenario.\n\n!pip install numpy pandas plotly seaborn scikit-learn\n\nRequirement already satisfied: numpy in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (1.24.3)\nRequirement already satisfied: pandas in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (1.5.3)\nRequirement already satisfied: plotly in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (5.9.0)\nRequirement already satisfied: seaborn in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (0.12.2)\nRequirement already satisfied: scikit-learn in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (1.3.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from pandas) (2022.7)\nRequirement already satisfied: tenacity&gt;=6.2.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from plotly) (8.2.2)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from seaborn) (3.7.1)\nRequirement already satisfied: scipy&gt;=1.5.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (1.0.5)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (4.25.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (23.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from matplotlib!=3.6.1,&gt;=3.1-&gt;seaborn) (3.0.9)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\frank\\anaconda3\\anaconda\\lib\\site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas) (1.16.0)\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plot\nimport seaborn as sns\nimport plotly.express as pxp\nimport plotly.graph_objs as gph\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\ndata = pd.read_csv('Customers.csv')\ndata.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnualIncome\nSpendingScore\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\ndata.drop('CustomerID', axis=1, inplace=True)\n\n\nplot.figure(figsize = (22, 10))\nplotnum = 1\n\nfor cols in ['Age', 'AnnualIncome', 'SpendingScore']:\n    if plotnum &lt;= 3:\n        axs = plot.subplot(1, 3, plotnum)\n        sns.distplot(data[cols])\n\n    plotnum += 1\n\nplot.tight_layout()\nplot.show()\n\nC:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_23260\\118834546.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data[cols])\nC:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_23260\\118834546.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data[cols])\nC:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_23260\\118834546.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data[cols])\n\n\n\n\n\n\nage_55above = data.Age[data.Age &gt;= 55]\nage_46_55 = data.Age[(data.Age &gt;= 46) & (data.Age &lt;= 55)]\nage_36_45 = data.Age[(data.Age &gt;= 36) & (data.Age &lt;= 45)]\nage_26_35 = data.Age[(data.Age &gt;= 26) & (data.Age &lt;= 35)]\nage_18_25 = data.Age[(data.Age &gt;= 18) & (data.Age &lt;= 25)]\n\n\nx_age_ax = ['18-25', '26-35', '36-45', '46-55', '55+']\ny_age_ax = [len(age_18_25.values), len(age_26_35.values), len(age_36_45.values), len(age_46_55.values),\n     len(age_55above.values)]\n\npxp.bar(data_frame = data, x = x_age_ax, y = y_age_ax, color = x_age_ax,\n       title = 'Count of customers per age group')\n\n\n                                                \n\n\n\nx_input = data.loc[:, ['Age', 'SpendingScore']].values\nwcss = []\nfor k in range(1, 12):\n    k_means = KMeans(n_clusters=k, init='k-means++')\n    k_means.fit(x_input)\n    wcss.append(k_means.inertia_)\n\nplot.figure(figsize=(15,8))\n\nplot.plot(range(1, 12), wcss, linewidth=2, marker='8')\nplot.title('Elbow methodn', fontsize=18)\nplot.xlabel('K')\nplot.ylabel('WCSS')\nplot.show()\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n\n\n\n\nk_means=KMeans(n_clusters=4)\nlabels=k_means.fit_predict(x_input)\nprint(k_means.cluster_centers_)\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\frank\\anaconda3\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n[[30.1754386  82.35087719]\n [27.61702128 49.14893617]\n [55.70833333 48.22916667]\n [43.29166667 15.02083333]]\n\n\n\nplot.figure(figsize = (16, 10))\nplot.scatter(x_input[:, 0], x_input[:, 1], c =\nk_means.labels_, s = 105)\nplot.scatter(k_means.cluster_centers_[:, 0],k_means.cluster_centers_[:, 1], color = 'red', s = 250)\nplot.title('Customers clustersn', fontsize = 20)\nplot.xlabel('Age')\nplot.ylabel('Spending Score')\nplot.show()"
  },
  {
    "objectID": "posts/Predictive Analysis/PredictiveAnalysis.html",
    "href": "posts/Predictive Analysis/PredictiveAnalysis.html",
    "title": "Topic 1 : Predictive Analysis To Predict Diagnosis of a Breast Tumor",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\nconda install -c conda-forge/label/cf202003 nodejs\n\n\npy -m pip install jupyterlab-quarto==0.1.45\n\n\n# Lester Anderson 5805 Machine Learning Blog\n\n\nBreast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed.\n\ndf = pd.read_csv('data.csv')\n\n\ndf.head(5) \n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows Ã— 32 columns\n\n\n\n\ndf.to_csv('data_clean_id.csv')\n\n\ndf.shape\n\n(569, 31)\n\n\n\ndf.info\n\n&lt;bound method DataFrame.info of     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0           M        17.99         10.38          122.80     1001.0   \n1           M        20.57         17.77          132.90     1326.0   \n2           M        19.69         21.25          130.00     1203.0   \n3           M        11.42         20.38           77.58      386.1   \n4           M        20.29         14.34          135.10     1297.0   \n..        ...          ...           ...             ...        ...   \n564         M        21.56         22.39          142.00     1479.0   \n565         M        20.13         28.25          131.20     1261.0   \n566         M        16.60         28.08          108.30      858.1   \n567         M        20.60         29.33          140.10     1265.0   \n568         B         7.76         24.54           47.92      181.0   \n\n     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0            0.11840           0.27760         0.30010              0.14710   \n1            0.08474           0.07864         0.08690              0.07017   \n2            0.10960           0.15990         0.19740              0.12790   \n3            0.14250           0.28390         0.24140              0.10520   \n4            0.10030           0.13280         0.19800              0.10430   \n..               ...               ...             ...                  ...   \n564          0.11100           0.11590         0.24390              0.13890   \n565          0.09780           0.10340         0.14400              0.09791   \n566          0.08455           0.10230         0.09251              0.05302   \n567          0.11780           0.27700         0.35140              0.15200   \n568          0.05263           0.04362         0.00000              0.00000   \n\n     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n0           0.2419  ...        25.380          17.33           184.60   \n1           0.1812  ...        24.990          23.41           158.80   \n2           0.2069  ...        23.570          25.53           152.50   \n3           0.2597  ...        14.910          26.50            98.87   \n4           0.1809  ...        22.540          16.67           152.20   \n..             ...  ...           ...            ...              ...   \n564         0.1726  ...        25.450          26.40           166.10   \n565         0.1752  ...        23.690          38.25           155.00   \n566         0.1590  ...        18.980          34.12           126.70   \n567         0.2397  ...        25.740          39.42           184.60   \n568         0.1587  ...         9.456          30.37            59.16   \n\n     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n0        2019.0           0.16220            0.66560           0.7119   \n1        1956.0           0.12380            0.18660           0.2416   \n2        1709.0           0.14440            0.42450           0.4504   \n3         567.7           0.20980            0.86630           0.6869   \n4        1575.0           0.13740            0.20500           0.4000   \n..          ...               ...                ...              ...   \n564      2027.0           0.14100            0.21130           0.4107   \n565      1731.0           0.11660            0.19220           0.3215   \n566      1124.0           0.11390            0.30940           0.3403   \n567      1821.0           0.16500            0.86810           0.9387   \n568       268.6           0.08996            0.06444           0.0000   \n\n     concave points_worst  symmetry_worst  fractal_dimension_worst  \n0                  0.2654          0.4601                  0.11890  \n1                  0.1860          0.2750                  0.08902  \n2                  0.2430          0.3613                  0.08758  \n3                  0.2575          0.6638                  0.17300  \n4                  0.1625          0.2364                  0.07678  \n..                    ...             ...                      ...  \n564                0.2216          0.2060                  0.07115  \n565                0.1628          0.2572                  0.06637  \n566                0.1418          0.2218                  0.07820  \n567                0.2650          0.4087                  0.12400  \n568                0.0000          0.2871                  0.07039  \n\n[569 rows x 31 columns]&gt;\n\n\n\ndf.isnull().any()\n\ndiagnosis                  False\nradius_mean                False\ntexture_mean               False\nperimeter_mean             False\narea_mean                  False\nsmoothness_mean            False\ncompactness_mean           False\nconcavity_mean             False\nconcave points_mean        False\nsymmetry_mean              False\nfractal_dimension_mean     False\nradius_se                  False\ntexture_se                 False\nperimeter_se               False\narea_se                    False\nsmoothness_se              False\ncompactness_se             False\nconcavity_se               False\nconcave points_se          False\nsymmetry_se                False\nfractal_dimension_se       False\nradius_worst               False\ntexture_worst              False\nperimeter_worst            False\narea_worst                 False\nsmoothness_worst           False\ncompactness_worst          False\nconcavity_worst            False\nconcave points_worst       False\nsymmetry_worst             False\nfractal_dimension_worst    False\ndtype: bool\n\n\n\ndf.diagnosis.unique()\n\narray(['M', 'B'], dtype=object)\n\n\n\ndf.to_csv('data_clean.csv')\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\n\n\nfrom scipy.stats import norm\n\n\nimport seaborn as sns\n\n\nplt.rcParams['figure.figsize'] = (15,8) \nplt.rcParams['axes.titlesize'] = 'large'\n\n\ndf = pd.read_csv('data_clean_id.csv', index_col=False)\ndf.drop('Unnamed: 0',axis=1, inplace=True)\ndf.head(3)\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.8\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.6\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.9\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.8\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.0\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.5\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n\n\n3 rows Ã— 32 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nid\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n3.037183e+07\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n...\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n1.250206e+08\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n...\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n8.670000e+03\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n...\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n8.692180e+05\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n...\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n9.060240e+05\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n...\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n8.813129e+06\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n...\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n9.113205e+08\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n...\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n8 rows Ã— 31 columns\n\n\n\n\ndf.skew()\n\nC:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_17564\\1665899112.py:1: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  df.skew()\n\n\nid                         6.473752\nradius_mean                0.942380\ntexture_mean               0.650450\nperimeter_mean             0.990650\narea_mean                  1.645732\nsmoothness_mean            0.456324\ncompactness_mean           1.190123\nconcavity_mean             1.401180\nconcave points_mean        1.171180\nsymmetry_mean              0.725609\nfractal_dimension_mean     1.304489\nradius_se                  3.088612\ntexture_se                 1.646444\nperimeter_se               3.443615\narea_se                    5.447186\nsmoothness_se              2.314450\ncompactness_se             1.902221\nconcavity_se               5.110463\nconcave points_se          1.444678\nsymmetry_se                2.195133\nfractal_dimension_se       3.923969\nradius_worst               1.103115\ntexture_worst              0.498321\nperimeter_worst            1.128164\narea_worst                 1.859373\nsmoothness_worst           0.415426\ncompactness_worst          1.473555\nconcavity_worst            1.150237\nconcave points_worst       0.492616\nsymmetry_worst             1.433928\nfractal_dimension_worst    1.662579\ndtype: float64\n\n\n\ndf.diagnosis.unique()\n\narray(['M', 'B'], dtype=object)\n\n\n\ndiag_gr = df.groupby('diagnosis', axis=0)\n\n\npd.DataFrame(diag_gr.size(), columns=['# of observations'])\npd.DataFrame(diag_gr.size(), columns=['# of observations'])\n\n\n\n\n\n\n\n\n# of observations\n\n\ndiagnosis\n\n\n\n\n\nB\n357\n\n\nM\n212"
  }
]